[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome EON Summer School 2023 (28.8-01.09-2023)",
    "section": "",
    "text": "Polsterberger Hubhaus 2022\n\n\n\n\n\n\n\nPolsterberger Hubhaus 2023"
  },
  {
    "objectID": "index.html#datasets",
    "href": "index.html#datasets",
    "title": "Welcome EON Summer School 2023 (28.8-01.09-2023)",
    "section": "Datasets",
    "text": "Datasets\n\nAirborne Laser Scanning (ALS) data ca. 2016\nUAV RGB & multispectral data 2022 2023\nDOP (20cm)\nPlanetScope Scene (R,G,B,NIR) covering the entire Harz from 24.08.2022\nDigitial elevation model (25m) from https://land.copernicus.eu/imagery-in-situ/eu-dem/eu-dem-v1.1 covering the entire Harz"
  },
  {
    "objectID": "index.html#sensors-equipment",
    "href": "index.html#sensors-equipment",
    "title": "Welcome EON Summer School 2023 (28.8-01.09-2023)",
    "section": "Sensors & Equipment",
    "text": "Sensors & Equipment\n\nHAWK:\n\nMavic 3 (Enterprise + Thermal + Multispectral)\nGNSS (Emlid, Alberding, Garmin)\nTablets (Android)\nForest Measurement Devices (diameter tapes, calipers, vertex, laser range finders, …)\nGeoSlam Mobile Laser Scanner\n\nUni Münster:\n\nMica Sens red edge (Multispektral)\nWIRIS thermal camera\nL1 Lidar (Dji)\nSony Alpha (42mp) RGB camera\nppm 10xx GNSS Sensor"
  },
  {
    "objectID": "mc_session/mc4.html",
    "href": "mc_session/mc4.html",
    "title": "Data Sources",
    "section": "",
    "text": "Station inside the dead Fir plot (Ecowitt Dashboard Login required)\n  \n  \n    \n     Station inside the clear cut (Ecowitt Dashboard Login required)\n  \n  \n    \n     Position Data of the climate stations\n  \n\n      \nData logger and measurement data"
  },
  {
    "objectID": "Machine_Learning_Session/ML_AOA.html",
    "href": "Machine_Learning_Session/ML_AOA.html",
    "title": "Machine learning for remote sensing applications",
    "section": "",
    "text": "In this tutorial we will go through the basic workflow of training machine learning models for spatial mapping based on remote sensing. To do this we will look at two case studies located in the MarburgOpenForest in Germany: one has the aim to produce a land cover map including different tree species; the other aims at producing a map of Leaf Area Index.\nBased on “default” models, we will further discuss the relevance of different validation strategies and the area of applicability.\n\n\nFor this tutorial we need the raster package for processing of the satellite data (note: needs to be replaced by terra soon) as well as the caret package as a wrapper for machine learning (here: randomForest) algorithms. Sf is used for handling of the training data available as vector data (polygons). Mapview is used for spatial visualization of the data. CAST will be used to account for spatial dependencies during model validation as well as for the estimation of the AOA.\n\nrm(list=ls())\n#major required packages:\nlibrary(devtools)\ninstall_github(\"HannaMeyer/CAST\")\nlibrary(terra)\nlibrary(caret)\nlibrary(mapview)\nlibrary(sf)\nlibrary(CAST)\nlibrary(tmap)"
  },
  {
    "objectID": "Machine_Learning_Session/ML_AOA.html#how-to-start",
    "href": "Machine_Learning_Session/ML_AOA.html#how-to-start",
    "title": "Machine learning for remote sensing applications",
    "section": "",
    "text": "For this tutorial we need the raster package for processing of the satellite data (note: needs to be replaced by terra soon) as well as the caret package as a wrapper for machine learning (here: randomForest) algorithms. Sf is used for handling of the training data available as vector data (polygons). Mapview is used for spatial visualization of the data. CAST will be used to account for spatial dependencies during model validation as well as for the estimation of the AOA.\n\nrm(list=ls())\n#major required packages:\nlibrary(devtools)\ninstall_github(\"HannaMeyer/CAST\")\nlibrary(terra)\nlibrary(caret)\nlibrary(mapview)\nlibrary(sf)\nlibrary(CAST)\nlibrary(tmap)"
  },
  {
    "objectID": "Machine_Learning_Session/ML_AOA.html#data-preparation",
    "href": "Machine_Learning_Session/ML_AOA.html#data-preparation",
    "title": "Machine learning for remote sensing applications",
    "section": "Data preparation",
    "text": "Data preparation\nTo start with, let’s load and explore the remote sensing raster data as well as the vector data that include the training sites.\n\nRaster data (predictor variables)\n\nmof_sen &lt;- rast(\"data/sentinel_uniwald.grd\")\nprint(mof_sen)\n\nclass       : SpatRaster \ndimensions  : 522, 588, 10  (nrow, ncol, nlyr)\nresolution  : 10, 10  (x, y)\nextent      : 474200, 480080, 5629540, 5634760  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=32 +datum=WGS84 +units=m +no_defs \nsource      : sentinel_uniwald.grd \nnames       : T32UM~1_B02, T32UM~1_B03, T32UM~1_B04, T32UM~1_B05, T32UM~1_B06, T32UM~1_B07, ... \nmin values  :         723,         514,         294,    341.8125,    396.9375,    440.8125, ... \nmax values  :        8325,        9087,       13810,   7368.7500,   8683.8125,   9602.3125, ... \n\n\nThe raster data contain a subset of the optical data from Sentinel-2 (see band information here: https://en.wikipedia.org/wiki/Sentinel-2) given in scaled reflectances (B02-B11). In addition,the NDVI was calculated. Let’s plot the data to get an idea how the variables look like.\n\nplot(mof_sen)\n\n\n\nplotRGB(mof_sen,r=3,g=2,b=1,stretch=\"lin\")\n\n\n\n\n\n\nVector data (Response variable)\nThe vector file is read as sf object. It contains the training sites that will be regarded here as a ground truth for the land cover classification.\n\ntrainSites &lt;- read_sf(\"data/trainingsites_LUC.gpkg\")\n\nUsing mapview we can visualize the aerial image channels in the geographical context and overlay it with the polygons. Click on the polygons to see which land cover class is assigned to a respective polygon.\n\nmapview(as(mof_sen[[1]],\"Raster\"), map.types = \"Esri.WorldImagery\")+\n  mapview(trainSites)\n\n\n\n\n\n\n\n\nDraw training samples and extract raster information\nIn order to train a machine learning model between the spectral properties and the land cover class, we first need to create a data frame that contains the predictor variables at the location of the training sites as well as the corresponding class information. However, using each pixel overlapped by a polygon would lead to a overly huge dataset, therefore, we first draw training samples from the polygon. Let’s use 1000 randomly sampled (within the polygons) pixels as training data set.\n\ntrainlocations &lt;- st_sample(trainSites,1000)\ntrainlocations &lt;- st_join(st_sf(trainlocations), trainSites)\nmapview(trainlocations)\n\n\n\n\n\n\nNext, we can extract the raster values for these locations. The resulting data frame contains the predictor variables for each training location that we can merged with the information on the land cover class from the sf object.\n\ntrainDat &lt;- extract(mof_sen, trainlocations, df=TRUE)\ntrainDat &lt;- data.frame(trainDat, trainlocations)\nhead(trainDat)\n\n  ID T32UMB_20170510T103031_B02 T32UMB_20170510T103031_B03\n1  1                       1102                       1094\n2  2                        871                        841\n3  3                        816                        762\n4  4                        979                       1011\n5  5                       1059                        901\n6  6                       1030                        789\n  T32UMB_20170510T103031_B04 T32UMB_20170510T103031_B05\n1                       1184                  1509.8750\n2                        530                  1048.0000\n3                        467                  1114.8125\n4                        892                  1426.3750\n5                        745                  1029.8750\n6                        720                   871.1875\n  T32UMB_20170510T103031_B06 T32UMB_20170510T103031_B07\n1                   2436.062                   2819.750\n2                   2456.250                   3003.125\n3                   2054.125                   2342.500\n4                   2594.438                   2948.750\n5                   1952.188                   2326.062\n6                   1289.562                   1534.000\n  T32UMB_20170510T103031_B08 T32UMB_20170510T103031_B11\n1                       2475                   2402.062\n2                       3501                   1651.000\n3                       2164                   1429.812\n4                       3092                   2142.188\n5                       1684                   1600.250\n6                       1280                   1350.312\n  T32UMB_20170510T103031_B12      NDVI id  LN     Type                 geometry\n1                  1730.8750 0.3528287 NA 110   Felder POINT (478270.6 5631418)\n2                   812.8125 0.7370380 77 306    Wiese   POINT (477161 5630701)\n3                   682.2500 0.6450019 NA   1    Eiche POINT (477831.1 5632362)\n4                  1395.6875 0.5522088 NA 101   Felder POINT (478405.5 5631393)\n5                  1039.2500 0.3865788 NA 204 Siedlung POINT (476575.6 5632390)\n6                   948.8750 0.2800000 NA 201 Siedlung POINT (476729.3 5632788)"
  },
  {
    "objectID": "Machine_Learning_Session/ML_AOA.html#model-training",
    "href": "Machine_Learning_Session/ML_AOA.html#model-training",
    "title": "Machine learning for remote sensing applications",
    "section": "Model training",
    "text": "Model training\n\nPredictors and response\nFor model training we need to define the predictor and response variables. As predictors we can use basically all information from the raster stack as we might assume they could all be meaningful for the differentiation between the land cover classes. As response variable we use the “Label” column of the data frame.\n\npredictors &lt;- names(mof_sen)\nresponse &lt;- \"Type\"\n\n\n\nA first “default” model\nWe then train a Random Forest model to lean how the classes can be distinguished based on the predictors (note: other algorithms would work as well. See https://topepo.github.io/caret/available-models.html for a list of algorithms available in caret). Caret’s train function is doing this job.\nSo let’s see how we can then train a “default” random forest model. We specify “rf” as method, indicating that a Random Forest is applied. We reduce the number of trees (ntree) to 75 to speed things up. Note that usually a larger number (&gt;250) is appropriate.\n\nmodel &lt;- train(trainDat[,predictors],\n               trainDat[,response],\n               method=\"rf\",\n               ntree=75)\nmodel\n\nRandom Forest \n\n1000 samples\n  10 predictor\n  10 classes: 'Buche', 'Duglasie', 'Eiche', 'Felder', 'Fichte', 'Laerche', 'Siedlung', 'Strasse', 'Wasser', 'Wiese' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 1000, 1000, 1000, 1000, 1000, 1000, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   2    0.8535466  0.8203271\n   6    0.8506646  0.8169128\n  10    0.8449483  0.8100697\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\n\nTo perform the classification we can then use the trained model and apply it to each pixel of the raster stack using the predict function.\n\nprediction &lt;- predict(mof_sen,model)\n\nThen we can then create a map with meaningful colors of the predicted land cover using the tmap package.\n\ncols &lt;- rev(c(\"palegreen\", \"blue\", \"grey\", \"red\", \"lightgreen\", \"forestgreen\", \"beige\",\"brown\",\"darkgreen\",\"yellowgreen\"))\n\ntm_shape(prediction) +\n  tm_raster(palette = cols,title = \"LUC\")+\n  tm_scale_bar(bg.color=\"white\",bg.alpha=0.75)+\n  tm_layout(legend.bg.color = \"white\",\n            legend.bg.alpha = 0.75)\n\n\n\n\nBased on this we can now discuss more advanced aspects of cross-validation for performance assessment as well as spatial variable selection strategies.\n\n\nModel training with spatial CV and variable selection\nBefore starting model training we can specify some control settings using trainControl. For hyperparameter tuning (mtry) as well as for error assessment we use a spatial cross-validation. Here, the training data are split into 5 folds by trying to resemble the geographic distance distribution required when predicting the entire area from the trainign data,\n\n## define prediction area:\nstudyArea &lt;- as.polygons(mof_sen, values = FALSE, na.all = TRUE) |&gt;\n    st_as_sf() |&gt;\n    st_transform(st_crs(trainlocations))|&gt;\n    st_union()\nmapview(studyArea)\n\n\n\n\n indices &lt;- knndm(trainlocations,studyArea,k=5)\ngd &lt;- geodist(trainlocations,studyArea,cvfolds = indices$indx_train )\nplot(gd)+ scale_x_log10(labels=round)\n\n\n\nctrl &lt;- trainControl(method=\"cv\", \n                     index = indices$indx_train,\n                     indexOut = indices$indx_test,\n                     savePredictions = TRUE)\n\nModel training is then again performed using caret’s train function. However we use a wrapper around it that is selecting the predictor variables which are relevant for making predictions to new spatial locations (forward feature selection, fss). We use the Kappa index as metric to select the best model.\n\n# train the model\nset.seed(100)\nmodel &lt;- ffs(trainDat[,predictors],\n             trainDat[,response],\n             method=\"rf\",\n             metric=\"Kappa\",\n             trControl=ctrl,\n             importance=TRUE,\n             ntree=75,\n             verbose=FALSE)\n\n\nprint(model)\n\nSelected Variables: \nT32UMB_20170510T103031_B05 T32UMB_20170510T103031_B06 NDVI T32UMB_20170510T103031_B11 T32UMB_20170510T103031_B03 T32UMB_20170510T103031_B08\n---\nRandom Forest \n\n1000 samples\n   6 predictor\n  10 classes: 'Buche', 'Duglasie', 'Eiche', 'Felder', 'Fichte', 'Laerche', 'Siedlung', 'Strasse', 'Wasser', 'Wiese' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 789, 842, 664, 850, 855 \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n  2     0.7778211  0.5794868\n  4     0.7430410  0.5320666\n  6     0.7794109  0.5876804\n\nKappa was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 6.\n\nplot(varImp(model))\n\n\n\n\n\n\nModel validation\nWhen we print the model (see above) we get a summary of the prediction performance as the average Kappa and Accuracy of the three spatial folds. Looking at all cross-validated predictions together we can get the “global” model performance.\n\n# get all cross-validated predictions:\ncvPredictions &lt;- model$pred[model$pred$mtry==model$bestTune$mtry,]\n# calculate cross table:\ntable(cvPredictions$pred,cvPredictions$obs)\n\n          \n           Buche Duglasie Eiche Felder Fichte Laerche Siedlung Strasse Wasser\n  Buche        7        1     0      1      0       0        0       0      0\n  Duglasie     0       16     0      0      5       0        0       0      0\n  Eiche        4        0     0      9      2       0        0       0      0\n  Felder       7        0     0    185      0       0        0       5      0\n  Fichte       0        2     0      0      0       0        0       0      0\n  Laerche      0        0     0      2      0       0        0       0      0\n  Siedlung     0        0     0      2      0       0        0       0      0\n  Strasse      0        0     0      7      0       0        0      24      0\n  Wasser       0        1     0      2      0       0        0       0      0\n  Wiese        0        0     0     15      0       0        0       3      0\n          \n           Wiese\n  Buche        0\n  Duglasie     0\n  Eiche        0\n  Felder       9\n  Fichte       1\n  Laerche      0\n  Siedlung     0\n  Strasse      1\n  Wasser       0\n  Wiese       58\n\n\n\n\nVisualize the final model predictions\n\nprediction &lt;- predict(mof_sen,model)\ncols &lt;- rev(c(\"palegreen\", \"blue\", \"grey\", \"red\", \"lightgreen\", \"forestgreen\", \"beige\",\"brown\",\"darkgreen\",\"yellowgreen\"))\n\ntm_shape(prediction) +\n  tm_raster(palette = cols,title = \"LUC\")+\n  tm_scale_bar(bg.color=\"white\",bg.alpha=0.75)+\n  tm_layout(legend.bg.color = \"white\",\n            legend.bg.alpha = 0.75)"
  },
  {
    "objectID": "Machine_Learning_Session/ML_AOA.html#area-of-applicability",
    "href": "Machine_Learning_Session/ML_AOA.html#area-of-applicability",
    "title": "Machine learning for remote sensing applications",
    "section": "Area of Applicability",
    "text": "Area of Applicability\nWe have seen that technically, the trained model can be applied to the entire area of interest (and beyond…as long as the sentinel predictors are available which they are, even globally). But we should assess if we SHOULD apply our model to the entire area. The model should only be applied to locations that feature predictor properties that are comparable to those of the training data. If dissimilarity to the training data is larger than the dissimmilarity within the training data, the model should not be applied to this location.\n\nAOA &lt;- aoa(mof_sen,model)\nplot(AOA$AOA)\n\n\n\n\nThe result of the aoa function has two layers: the dissimilarity index (DI) and the area of applicability (AOA). The DI can take values from 0 to Inf, where 0 means that a location has predictor properties that are identical to properties observed in the training data. With increasing values the dissimilarity increases. The AOA has only two values: 0 and 1. 0 means that a location is outside the area of applicability, 1 means that the model is inside the area of applicability."
  },
  {
    "objectID": "Machine_Learning_Session/ML_AOA.html#prepare-data",
    "href": "Machine_Learning_Session/ML_AOA.html#prepare-data",
    "title": "Machine learning for remote sensing applications",
    "section": "Prepare data",
    "text": "Prepare data\n\nmof_sen &lt;- rast(\"data/sentinel_uniwald.grd\")\nLAIdat &lt;- st_read(\"data/trainingsites_LAI.gpkg\")\n\nReading layer `trainingsites_LAI' from data source \n  `/home/creu/edu/summerschool2023/EON2023/Machine_Learning_Session/data/trainingsites_LAI.gpkg' \n  using driver `GPKG'\nSimple feature collection with 67 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 476350 ymin: 5631537 xmax: 478075 ymax: 5632765\nProjected CRS: WGS 84 / UTM zone 32N\n\ntrainDat &lt;- extract(mof_sen,LAIdat,na.rm=TRUE)\ntrainDat$LAI &lt;- LAIdat$LAI\n\n\nmeanmodel &lt;- mof_sen[[1]]\nvalues(meanmodel) &lt;- mean(trainDat$LAI)\nplot(meanmodel)\n\n\n\nrandommodel &lt;- mof_sen[[1]]\nvalues(randommodel)&lt;- runif(ncell(randommodel),min = 0,4)\n\nplot(randommodel)"
  },
  {
    "objectID": "Machine_Learning_Session/ML_AOA.html#a-simple-linear-model",
    "href": "Machine_Learning_Session/ML_AOA.html#a-simple-linear-model",
    "title": "Machine learning for remote sensing applications",
    "section": "A simple linear model",
    "text": "A simple linear model\nAs a simple first approach we might develop a linear model. Let’s assume a linear relationship between the NDVI and the LAI\n\nplot(trainDat$NDVI,trainDat$LAI)\nmodel_lm &lt;- lm(LAI~NDVI,data=trainDat)\nsummary(model_lm)\n\n\nCall:\nlm(formula = LAI ~ NDVI, data = trainDat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.87314 -0.52143 -0.03363  0.63668  2.25252 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -0.8518     1.4732  -0.578  0.56515   \nNDVI          6.8433     2.3160   2.955  0.00435 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8887 on 65 degrees of freedom\nMultiple R-squared:  0.1184,    Adjusted R-squared:  0.1049 \nF-statistic: 8.731 on 1 and 65 DF,  p-value: 0.004354\n\nabline(model_lm,col=\"red\")\n\n\n\nprediction_LAI &lt;- predict(mof_sen,model_lm,na.rm=T)\nplot(prediction_LAI)\n\n\n\nlimodelpred &lt;- -0.8518+mof_sen$NDVI*6.8433\nmapview(as(limodelpred,\"Raster\"))"
  },
  {
    "objectID": "Machine_Learning_Session/ML_AOA.html#the-machine-learning-way",
    "href": "Machine_Learning_Session/ML_AOA.html#the-machine-learning-way",
    "title": "Machine learning for remote sensing applications",
    "section": "The machine learning way",
    "text": "The machine learning way\n\nDefine CV folds\nLet’s use the NNDM cross-validation approach.\n\nstudyArea &lt;- as.polygons(mof_sen, values = FALSE, na.all = TRUE) |&gt;\n    st_as_sf() |&gt;\n    st_transform(st_crs(LAIdat))|&gt;\n    st_union()\n\nnndm_folds &lt;- knndm(LAIdat,studyArea,k=3)\n\nLet’s explore the geodistance\n\ngd &lt;- geodist(LAIdat,studyArea,cvfolds = nndm_folds$indx_test)\nplot(gd)\n\n\n\n\n\n\nModel training\n\nctrl &lt;- trainControl(method=\"cv\",\n                     index=nndm_folds$indx_train,\n                     indexOut = nndm_folds$indx_test,\n                    savePredictions = \"all\")\n\n\nmodel_rf &lt;- train(trainDat[,2:11],\n                  trainDat$LAI,\n                  method = \"rf\")\n\n\n\nmodel &lt;- ffs(trainDat[,predictors],\n             trainDat$LAI,\n             method=\"rf\",\n             trControl = ctrl,\n             importance=TRUE,\n             verbose=FALSE)\n\n\nmodel\n\nSelected Variables: \nT32UMB_20170510T103031_B07 T32UMB_20170510T103031_B08 NDVI T32UMB_20170510T103031_B06\n---\nRandom Forest \n\n67 samples\n 4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 41, 44, 49 \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared   MAE      \n  2     0.8517638  0.1996342  0.6937305\n  3     0.8593777  0.1935101  0.7009237\n  4     0.8597842  0.1977293  0.7027634\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 2.\n\n\n\n\nLAI prediction\nLet’s then use the trained model for prediction.\n\nLAIprediction &lt;- predict(mof_sen,model)\nplot(LAIprediction)\n\n\n\n\n\nQuestion?! Why does it look so different than the linear model?\n\n\n\nAOA estimation\n\nAOA &lt;- aoa(mof_sen,model_rf)\nplot(AOA$AOA)"
  },
  {
    "objectID": "paul_tutorial/02_ValidationDataCollection.html",
    "href": "paul_tutorial/02_ValidationDataCollection.html",
    "title": "Collection of validation data in the context of remote sensing based forest monitoring",
    "section": "",
    "text": "Introduction\nIn this tutorial we will explore the principles of design-based sampling. The simulation part is based on a presentation of Gerad Heuveling from Wageningen University, which he gave in the OpenGeoHub Summer School[https://opengeohub.org/summer-school/ogh-summer-school-2021/].\n\nLearn how to draw a spatial random sample\nLearn how to draw a systematic grid for a given area of interest\nRun a simulation for design-based sampling\n\n\n\nData sets\nFor demonstration purposes we will work with a map of forest above ground biomass (AGB) produced by the Joint Research Center(JRC) for the European Union European Commission (Joint Research Centre (JRC) (2020) http://data.europa.eu/89h/d1fdf7aa-df33-49af-b7d5-40d226ec0da3.)\nTo provide a synthetic example we will assume that this map (agb_pop) is an error free representation of the population. Additionally we use a second map (agb_model) compiled using a machine learning model (RF) also depicting the AGB distribution.\n\nnp_boundary = st_transform(st_read(\"data/nlp-harz_aussengrenze.gpkg\"),25832)\n\nReading layer `nlp-harz_aussengrenze' from data source \n  `/home/creu/edu/summerschool2023/EON2023/paul_tutorial/data/nlp-harz_aussengrenze.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 591196.6 ymin: 5725081 xmax: 619212.6 ymax: 5751232\nProjected CRS: WGS 84 / UTM zone 32N\n\nagb_pop &lt;- raster(\"data/agb_np_harz_truth.tif\")\n\nagb_model &lt;-raster(\"data/agb_np_harz_model.tif\")\n\nIf we assume the \\(z(x_i)=\\) agb.pop to be an exact representation of the population we can calculate the Root mean Square Error (RMSE) as the difference between the model predictions \\(\\hat{z(x_i)}\\) and the population map with:\n\\[\nRMSE = \\sqrt{\\frac{1}{N}\\sum{(z(x_{i})-\\hat{z}(x_{i}))^2}}\n\\]\n\nRMSE_pop = sqrt(cellStats((agb_pop-agb_model)^2, mean))\n\nBy looking at the difference from the “true” AGB and the difference we get a true RMSE of 41.23 t/ha.\n\n\nCollect a random sample\nSince we know the true RMSE, we can test if a random sample estimate has a similar RMSE. We start with a random sample with \\(n=100\\) sample points.\n\nn=100\np1 = st_sample(np_boundary,size=n)\nplot(st_geometry(np_boundary))\nplot(p1,add=TRUE,pch=1)\n\n\n\n\nWe can now extract the population values and the model values at the sample locations and calculate the RMSE for all sample points.\n\nsample &lt;- raster::extract((agb_pop-agb_model),as_Spatial(p1))\nRMSE_est &lt;- sqrt(mean((sample)^2,na.rm=T))\n\nThe random sample estimates the RMSE as 43.4.\nBut is this an unbiased estimate?\n\n\nSimulation of many random samples\nTo check if our sample based estimates are unbiased we will repeat the sampling \\(k\\) times.\n\ndif &lt;- as((agb_pop-agb_model), 'SpatialGridDataFrame')\nseed&lt;- 12324\n\n\nk &lt;- 500\nn &lt;- 50\nRMSE &lt;- rep(0,k) \n\nfor (i in 1:k) {\n  #print(i)\n  p1 = spsample(as_Spatial(np_boundary),n=n,type='random')\n  crs(p1)&lt;-crs(dif)\n  #sample &lt;- raster::extract((agb_pop-agb_model),p1)\n  error&lt;-over(p1,dif)$layer\n  RMSE[i] &lt;- sqrt(mean((error)^2,na.rm=T))\n}\n\ndf &lt;- data.frame(x=RMSE, y=rep('a',k))\n\nggplot(data=df,aes(x=x))+\n  geom_density(data=subset(df,y=='a'),\n               fill='blue', alpha=0.5)+\n  xlab('RMSE')+geom_vline(xintercept=RMSE_pop,linewidth=1.5,\n                          color ='black', linetype='longdash')+\n  geom_vline(xintercept=mean(df$x),size=1.5,\n                          color ='black')\n\n\n\n\nWe see that the true RMSE and the mean of the \\(k\\) simulation runs are almost equal. Thus, we can assume an unbiased estimate of the RMSE.\nBut how does the sample size \\(n\\) affects the accuracy?\n\nk &lt;- 500\nn &lt;- 100\nRMSE_2 &lt;- rep(0,k) \n\nfor (i in 1:k) {\n  #print(i)\n  p1 = spsample(as_Spatial(np_boundary),n=n,type='random')\n  crs(p1)&lt;-crs(dif)\n  #sample &lt;- raster::extract((agb_pop-agb_model),p1)\n  error&lt;-over(p1,dif)$layer\n  RMSE_2[i] &lt;- sqrt(mean((error)^2,na.rm=T))\n}\n\ndf_2 &lt;- data.frame(x=RMSE_2, y=rep('b',k))\ndf&lt;-rbind(df,df_2)\n\nggplot(data=df,aes(x=x,fill=y))+\n  geom_density(alpha=0.5)+\n  scale_fill_discrete(labels=c('Random, n=50', 'Random, n=100'))+\n  xlab('RMSE')+geom_vline(xintercept=RMSE_pop,size=1.5,\n                          color ='black', linetype='longdash')+\n  geom_vline(xintercept=mean(df$x),size=1.5,\n                          color ='black')\n\n\n\n\nWe see that the precision of the esimtates is increased. How much did the uncertainty decrease when we increase the sample size from \\(n=50\\) to \\(n=100\\)?\n\nsd(RMSE_2)/sd(RMSE)\n\n[1] 0.7069502\n\n\n\n\nSystematic sampling\nInstead of a random sampling, systematic designs are more common in forest inventories for the following reasons:\n\nEasy to establish and to document\nEnsures a balanced spatial coverage\n\n\np1 = spsample(as_Spatial(np_boundary),n=n,type='regular')\n\nplot(np_boundary$geom)\nplot(p1, add=T)\n\n\n\n\n\nk &lt;- 500\nn &lt;- 100\nRMSE_3 &lt;- rep(0,k) \n\nfor (i in 1:k) {\n  #print(i)\n  p1 = spsample(as_Spatial(np_boundary),n=n,type='regular')\n  crs(p1)&lt;-crs(dif)\n  error&lt;-over(p1,dif)$layer\n  RMSE_3[i] &lt;- sqrt(mean((error)^2,na.rm=T))\n}\n\ndf_3&lt;- data.frame(x=RMSE_3, y=rep('c',k))\ndf&lt;-rbind(df,df_3)\n\nggplot(data=df,aes(x=x, fill=y))+\n  geom_density(alpha=0.5)+\n  scale_fill_discrete(labels=c('Random, n=50', 'Random, n=100','Systematic, n=100'))+\n  xlab('RMSE')+geom_vline(xintercept=RMSE_pop,size=1.5,\n                          color ='black', linetype='longdash')+\n  geom_vline(xintercept=mean(df$x),size=1.5,\n                       color ='black')"
  },
  {
    "objectID": "mc_session/mc1.html",
    "href": "mc_session/mc1.html",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "",
    "text": "froggit shop [DE]\n  \n  \n    \n     ecowitt shop [US]\n  \n  \n    \n     Fine Offset\n  \n\n      \nThe sensors from Fine Offset are re-branded and partly modified by the resellers. This article deals with sensors from the german re-seller froggit and the US re-seller ecowitt. More precise the DP-/GW SmartHubs WiFi Gateway with temperature, humidity & Pressure which is developed by fine offset. The unique selling point of the LoRa-Wifi gateway is the extraordinarily extensive possibility of connecting radio-bound sensors."
  },
  {
    "objectID": "mc_session/mc1.html#calibration-concept",
    "href": "mc_session/mc1.html#calibration-concept",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Calibration Concept",
    "text": "Calibration Concept\nThe low budget sensors are usually lacking of a stable measurement quality. To obtain reliable micro climate data a two step calibration process is suggested.\n\nThe measurements of all sensors (preferably in a climate chamber) will be statistically analysed to identify sensor which produce systematic and significant outliers.\nThe sensors are calibrated against an operational running high price reference station in the field.\n\n\n\n\n\n\n\nFuture Calibration Plans\n\n\n\n\n\nFor the future a machine learning approach including the radiation, azimuth, temperature and humidity as predictors for the calibrated temperature as the response variable will be used as an rolling calibration tool."
  },
  {
    "objectID": "mc_session/mc1.html#switching-scheme",
    "href": "mc_session/mc1.html#switching-scheme",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Switching scheme",
    "text": "Switching scheme\nThe battery box has a very simple design. Besides the cabling, it contains a solar charge regulator, a fuse panel for the protection of the consumers and an AGM 120aH battery.\n ## Components * Sealable, durable Wham Bam Heavy Duty Box, 62 L, 59,5 x 40 x 37 cm, PP Recycling Plastic Wham Bam Box. The “Wham Bam Box” made of recycled PP plastic was chosen for its extreme mechanical strength and almost complete biochemical resistance. The bad temperature spectrum for thermal stability is from approx. -10 -140 °C., it is acid and alkali resistant and waterproof. By additionally equipping the box with a fire protection mat, the almost airtight closure offers a virtually complete reduction of fire load inside and outside the box. * 12V deep-cycle battery BSA Audio Solar Technologie 120 Ah 12V C100 * 3 x Neutrik powerCON TRUE1 NAC3FPX outlets and Neutrik SCNAC-FPX sealing cover. * Fuse Box for car fuses up to max. 15A per fuse, maximum 30A per fuse box, With sealed cover, splash-proof, Material: PA6.6, 12 connections on the side * Nominal voltage: 32 V/DC * Nominal current (per output): 15 A * Temperature range: -20 - +85 °C * Connections: Flat plug 8x 6,3 x 0,8 mm lateral * Solar charge controller, 20A (ALLPOWERS, available from various brands) Specification ALLPOWERS"
  },
  {
    "objectID": "mc_session/mc1.html#wiring",
    "href": "mc_session/mc1.html#wiring",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Wiring",
    "text": "Wiring\n\nBattery to solar charger:\n\nPole terminal connectors (+ and -)\n6 mm2 cables (red and black)\n2 x Crimp cable shoes\n\nSolar panel to solar charger\n\nMC4 photovoltaic connectors (+ and -) Weidemüller\n6 mm2 cables (red and black)\n2x Crimp cable shoes\n\nSolar charger fuse box outlets\n\n6 x 1,5 mm2 cables, red\n6 x 1/4’’ FASTON terminals Fuse Box\n3 x 1,5 mm2 cables, black\n2 x Crimp cable shoes (holding 3 wires)\n6 x 6,35mm / 1/4’’ crimp FASTON terminals\n\n\nPlease note the following points: * Silicone cables, solar cables, plugs and fuse box fulfills industry standards. All cable lugs are crimped and checked. * The cable lugs are not screwed to the charging cables with cable lugs but through the crimp connection with the end sleeve. * A main fuse (e.g. 40A automatic circuit breaker) must be installed\nSee also the figure below."
  },
  {
    "objectID": "mc_session/mc1.html#mounting",
    "href": "mc_session/mc1.html#mounting",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Mounting",
    "text": "Mounting\n\nOutlets: 6x M3 screw (12mm), washers and nuts\nSolar connectors: 2 x waterproof cable glands\nSolar charger and fuse box:\n\nWooden plate, glued to the box\n4 screws for Solar Charge Controller\n4 screws for fuse box Cable lugs and plugs are covered with self-vulcanizing tape and additionally insulated."
  },
  {
    "objectID": "mc_session/mc1.html#station-setup-in-the-field",
    "href": "mc_session/mc1.html#station-setup-in-the-field",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Station setup in the field",
    "text": "Station setup in the field\nFor safe operation, the following points must be taken into account when setting up the box:\n1.) The box must be placed horizontally. Preferable at on a clearing to reduce impacts of falling branches or similar.  2.) One square meter around the box must be cleared of any vegetation and the A-horizon (depending on the slope, even more).\n 3.) Around this area a further strip with a diameter of at least 1 meter must also be cleared of organic material, especially leaves. Dig up the A-horizon and exclude roots and organic stuff. Note that the wiring sections must also be cleared of combustible organic material.\n 4.) Check cables and screws for proper seating and integrity.\n\n5.) Check proper installation of the solar panel. Mount the panel on a simple wooden slat attached to the frame to avoid damage to the protective foil on the back. Such damage will destroy the panel.\n\n6.) Attach the solar connectors to the panel. This avoids ground contact and provides good weather protection. This can be done very easily by threading cable ties through the plugs and the junction box. {% include figure image_path=“../images/battery_box/07_solar_plugs.jpg” alt=“Attach the solar connectors to the panel.” %}  7.) Finally, the box should be secured against unauthorized or accidental opening. For this purpose there is a steel cable with a number lock, which is to be attached in the way it is placed there."
  },
  {
    "objectID": "mc_session/mc1.html#final-check",
    "href": "mc_session/mc1.html#final-check",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Final check",
    "text": "Final check\n\nAll contacts and cables must be checked for proper seating and integrity. Especially the charging cables on the battery must be screwed tightly.\nAll cables are to be laid without tension.\nThe solar cables are to be laid separately to avoid a short circuit, so that an animal crossing etc. does not cause them to come into contact.\nThe box is secured and tight."
  },
  {
    "objectID": "mc_session/mc1.html#risk-assessment",
    "href": "mc_session/mc1.html#risk-assessment",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Risk Assessment",
    "text": "Risk Assessment\nHere you find the preliminary risk assesment for the installation and operation of 12 V solar power based energy supply units and measuring sensor systems."
  },
  {
    "objectID": "mc_session/mc3.html",
    "href": "mc_session/mc3.html",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "The use of quantitative methods, especially statistical methods, is of considerable importance for describing and explaining spatial patterns (e.g. landscape ecology). The central concept on which these methods are based is that of proximity, or location in relation to each other."
  },
  {
    "objectID": "mc_session/mc3.html#distance-and-data-representation",
    "href": "mc_session/mc3.html#distance-and-data-representation",
    "title": "Spatial Interpolation",
    "section": "Distance and data representation",
    "text": "Distance and data representation\nLet’s take a closer look at proximity, which is mentioned frequently. What exactly is it? How can proximity/neighborliness be expressed in such a way that the space becomes meaningful?\nIn general, spatial relationships are described in terms of neighborhoods (positional) and distances (metric). In spatial analysis or prediction, however, it is important to be able to name the spatial influence, i.e. the evaluation or weighting of this relationship, either qualitatively or quantitatively. Tobler did this for a specific objective by stating that “near” is more important than “far”. But what about in other cases? The challenge is that spatial influence can only be measured directly in exceptional cases. There are many ways to estimate it, however.\n\nNeighborhood\nNeighborhood is perhaps the most important concept. Higher dimensional geo-objects can be considered neighboring if they touch each other, e.g. neighboring countries. For zero-dimensional objects (points), the most common approach is to use distance in combination with a number of points to determine neighborhood.\n\n\nDistance\nProximity or neighborhood analyses are often concerned with areas of influence or catchment areas, i.e. spatial patterns of effects or processes.\nThis section discusses some methods for calculating distances between spatial objects. Because of the different ways of discretizing space, we must make the – already familiar – distinction between vector and raster data models.\nInitially, it is often useful to work without spatially restrictive conditions in a first analysis, e.g. when this information is missing. The term “proximity” inherently implies a certain imprecision. Qualitative terms that can be used for this are: “near”, “far” or “in the neighborhood of”. Representation and data-driven analysis require these terms to be objectified and operationalized. So, this metric must be based on a distance concept, e.g. Euclidean distance or travel times. In a second interpretative step, we must decide which units define this type of proximity. In terms of the objective of a question, there are only suitable and less-suitable measures; there is no correct or incorrect. Therefore, it is critical to define a meaningful neighborhood relationship for the objects under investigation."
  },
  {
    "objectID": "mc_session/mc3.html#filling-spatial-gaps",
    "href": "mc_session/mc3.html#filling-spatial-gaps",
    "title": "Spatial Interpolation",
    "section": "Filling spatial gaps",
    "text": "Filling spatial gaps\nNow that we have learned the basic concepts of distance, neighborhood and filling spatial gaps, let’s take a look at interpolating or predicting values in space.\nFor many decades, deterministic interpolation techniques (inverse distance weighting, nearest neighbor, kriging) have been the most popular spatial interpolation techniques. External drift kriging and regression kriging, in particular, are fundamental techniques that use spatial autocorrelation and covariate information, i.e. sophisticated regression statistics.\nMachine learning algorithms like random forest have become very popular for spatial environmental prediction. One major reason for this is that they are can take into account non-linear and complex relationships, i.e. compensate for certain disadvantages that are present in the usual regression methods."
  },
  {
    "objectID": "mc_session/mc3.html#proximity-concepts",
    "href": "mc_session/mc3.html#proximity-concepts",
    "title": "Spatial Interpolation",
    "section": "Proximity concepts",
    "text": "Proximity concepts\n\nVoronoi polygons – dividing space geometrically\nVoronoi polygons (aka Thiessen polygons) are an elementary method for geometrically determining proximity or neighborhoods. Voronoi polygons (see figure below) divide an area into regions that are closest to a given point in a set of irregularly distributed points. In two dimensions, a Voronoi polygon encompasses an area around a point, such that every spatial point within the Voronoi polygon is closer to this point than to any other point in the set. Such constructs can also be formed in higher dimensions, giving rise to Voronoi polyhedra.\n&lt;/frame&gt;\n&lt;iframe width=\"780\" height=\"500\" src=\"https://geomoer.github.io/geoAI//assets/images/unit01/suisse6.html\" title=\"Interpol\"&gt;\n\n\nThe blue dots are a typical example of irregularly distributed points in space – in this case, rain gauges in Switzerland. The overlaid polygons are the corresponding Voronoi segments that define the corresponding closest geometrical areas (gisma 2021)“\n\n\nSince Voronoi polygons correspond to an organizational principle frequently observed in both nature (e.g. plant cells) and in the spatial sciences (e.g. central places , according to Christaller), there are manifold possible applications. Two things must be assumed, however: First, that nothing else is known about the space between the sampled locations and, second, that the boundary line between two samples is incomplete idea.\nVoronoi polygons can also be used to delineate catchment areas of shops, service facilities or wells, like in the example of the Soho cholera outbreak. Please note that within a polygon, one of the spatial features is isomorphic, i.e. the spatial features are identical.\nBut what if we know more about the spatial relationships of the features? Let’s have a look at some crucial concepts.\n\n\nSpatial interpolation of data\nSpatially interpolating data points provides us with a modeled quasi-continuous estimation of features under the corresponding assumptions. But what is spatial interpolation? Essentially, this means using known values to calculate neighboring values that are unknown. Most of these techniques are among the most complex methods of spatial analysis, so we will deliberately limit ourselves here to a basic overview of the methods. Some of the best-known and common interpolation methods found in spatial sciences are nearest neighbor inverse distance, spline interpolations, kriging, and regression methods.\n\n\nContinously filling the gaps by interpolation\nTo get started, take a look at the following figure, which shows six different interpolation methods to derive the spatial distribution of precipitation in Switzerland (in addition to the overlaid Voronoi tessellation).\n\n\n\nThe blue dots are a typical example of irregularly distributed points in space – in this case, rain gauges in Switzerland. The size of each dot corresponds to the amount of precipitation in mm. The overlaid polygons are the corresponding Voronoi segments that define the corresponding closest geometrical areas (gisma 2021)” top left: Nearest neighbor interpolation based on 3-5 nearest neighbors, top right: Inverse Distance weighting (IDW) interpolation method middle left: AutoKriging with no additional parameters, middle right: Thin plate spline regression interpolation method bottom left: Triangular irregular net (TIN) surface interpolation, bottom right: additive model (GAM) interpolation\n\n\nIn the example of precipitation in Switzerland, the positions of the weather stations are fixed and cannot be freely chosen.\nWhen choosing an appropriate interpolation method, we need to pay attention to several properties of the samples (distribution and properties of the measurement points):\n\nRepresentativeness of measurement points: The sample should represent the phenomenon being analyzed in all of its manifestations.\nHomogeneity of measurement points: The spatial interdependence of the data is a very important basic requirement for further meaningful analysis.\nSpatial distribution of measurement points: The spatial distribution is of great importance. It can be completely random, regular or clustered.\nNumber of measurement points: The number of measurement points depends on the phenomenon and the area. In most cases, the choice of sample size is subject to practical limitations.\n\nWhat makes things even more complex is that these four factors – representativeness, homogeneity, spatial distribution and size – are all interrelated. For example, a sample size of 5 measuring stations for estimating precipitation for all of Switzerland is hardly meaningful and therefore not representative. Equally unrepresentative would be selecting every measuring station in German-speaking Switzerland to estimate precipitation for the entire country. In this case, the number alone might be sufficient, but the spatial distribution would not be. If we select every station at an altitude below 750 m asl, the sample could be correct in terms of both size and spatial distribution, but the phenomenon is not homogeneously represented in the sample. An estimate based on this sample would be clearly distorted, especially in areas above 750 m asl. In practice, virtually every natural spatially-continuous phenomenon is governed by stochastic fluctuations, so, mathematically speaking, it can only be described in approximate terms.\n\n\nMachine learning\nMachine learning (ML) methods such as random forest can also produce spatial and temporal predictions (i.e. produce maps from point observations). These methods are particularly robust because they take spatial autocorrelation into account, which can improve predictions or interpolations by adding geographic distances. This ultimately leads to better maps with much more complex relationships and dependencies.\nIn the simplest case, the results are comparable to the well-known model-based geostatistics. The advantage of ML methods over model-based geostatistics, however, is that they make fewer assumptions, can take non-linearities into account and are easier to automate.\n\n\n\nThe original dataset (top left) is a terrain model reduced to 8 meters with 48384 single pixels. For interpolation, 1448 points were randomly drawn and interpolated with conventional kriging (top right), support vector machines (SVM) (middle left), neural networks (middle right), and two variants of random forest (bottom row). In each method, only the distance of the drawn points is used as a dependency.\n\n\n\nEach interpolation method was applied using the “default” settings. Tuning could possibly lead to significant changes in all of them. Fascinatingly, the error measures correlate to the visual results: Kriging and the neural network show the best performance, followed by the random forest models and the support-vector machine.\n\n\n\nmodel\ntotal_error\nmean_error\nsd_error\n\n\n\n\nKriging\n15797773.0\n54.2\n67.9\n\n\nNeural Network\n19772241.0\n67.8\n80.5\n\n\nRandom Forest\n20540628.1\n70.4\n82.5\n\n\nNormalized Random Forest\n20597969.8\n70.6\n82.7\n\n\nSupport Vector Machine\n21152987.7\n72.5\n68.3\n\n\n\n\n\nAdditional references\nGet the Most Out of AI, Machine Learning, and Deep Learning Part 1 (10:52) and Part 2 (13:18)\nWhy You Should NOT Learn Machine Learning! (6:17)\nGeoAI: Machine Learning meets ArcGIS (8:50)"
  },
  {
    "objectID": "mc_session/mc3.html#hands-on-our-data",
    "href": "mc_session/mc3.html#hands-on-our-data",
    "title": "Spatial Interpolation",
    "section": "Hands on our data",
    "text": "Hands on our data\n\nSetup the environment\nPlease download the data from the repository or take the USB-stick\n\n#------------------------------------------------------------------------------\n# Author: creuden@gmail.com\n# Description:  interpolates the air temp\n# Copyright:GPL (&gt;= 3)  Date: 2023-08-28 \n#------------------------------------------------------------------------------\n\n# 0 ---- project setup ----\n\n# load packages (if not installed please install via install.packages())\nlibrary(\"raster\")\nlibrary(\"terra\")\nlibrary(\"sp\")\nlibrary(\"sf\")\nlibrary(\"dplyr\")\nlibrary(\"lwgeom\")\nlibrary(\"readxl\")\nlibrary(\"highfrequency\")\nlibrary(\"tidyverse\")\nlibrary(\"rprojroot\")\nlibrary(\"tibble\")\nlibrary(\"xts\")\nlibrary(\"data.table\")\nlibrary(\"mapview\")\n\n# create a string containing the current working directory\nwd=paste0(find_rstudio_root_file(),\"/mc_session/data/\")\n\n# define time period to aggregate temp dat\ntime_period = 3\n\n# multiplication factor for blowing up the Copernicus DEM\nblow_fac = 15\n\n# reference system as proj4 string for old SP package related stuff\ncrs = raster::crs(\"+proj=utm +zone=33 +datum=WGS84 +units=m +no_defs\")\n\n# Copernicus DEM (https://land.copernicus.eu/imagery-in-situ/eu-dem/eu-dem-v1.1)\nfnDTM = paste0(wd,\"copernicus_DEM.tif\")  \n\n# Weather Data adapt if you download a new file \n# https://www.ecowitt.net/home/index?id=20166)\n# https://www.ecowitt.net/home/index?id=149300\nfn_dataFC29 = paste0(wd,\"all_GW1000A-WIFIFC29(202308270000-202308292319).xlsx\")\nfn_dataDB2F =paste0(wd,\"all_GW1000A-WIFIDB2F(202308270000-202308292219).xlsx\")\n\n# station data as derived by the field group\nfn_pos_data= paste0(wd,\"stations_prelim.shp\")\n\n# arbitrary plot borders just digitized for getting a limiting border of the plot area\nfn_area =paste0(wd,\"plot.shp\")\n\n# rds file for saving the cleaned up weather data\ncleandata = paste0(wd,\"climdata.RDS\")\n\n# 1 ---- read data ----\n# read DEM data\nDTM = terra::rast(fnDTM) # DTM.\n# increase resolution by 15\nDTM=disagg(DTM, fact=c(blow_fac, blow_fac)) \n# cast to SpatialPixelsDataFrame\nDTM.spdf = as(raster(DTM),'SpatialPixelsDataFrame')\n# add colname altitude necessary for latter use during kriging\ncolnames(DTM.spdf@data) = \"altitude\"\n# fix reference system\ncrs(DTM.spdf)=crs\n# read station position data\n pos=st_read(fn_pos_data)\n\nReading layer `stations_prelim' from data source \n  `/home/creu/edu/summerschool2023/EON2023/mc_session/data/stations_prelim.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 14 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 183055.8 ymin: 5748366 xmax: 183170.3 ymax: 5748499\nProjected CRS: WGS 84 / UTM zone 33N\n\n # read station position data\narea=st_read(fn_area)\n\nReading layer `plot' from data source \n  `/home/creu/edu/summerschool2023/EON2023/mc_session/data/plot.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 10.40154 ymin: 51.79506 xmax: 10.40658 ymax: 51.79803\nGeodetic CRS:  WGS 84\n\n# reproject the dataset to the project crs\narea=st_transform(area,crs)\n# read temperature data we need to skip row 1 due to excel format\nclim_dataFC29 = as_tibble(read_excel(fn_dataFC29, skip = 1)) \nclim_dataDB2F = as_tibble(read_excel(fn_dataDB2F, skip = 1))\n\n\n\nCleaning data\nWe need to do an ugly cleaning job. This is basically the most cumbersome part of dealing with data analysis.\n\n# select the required cols\ntempFC29 = clim_dataFC29 %&gt;% dplyr::select(c(1,2,32,36,40,44,48))\ntempDB2F = clim_dataDB2F %&gt;% dplyr::select(c(1,25,29,33,37,41,45,49,53))\n# rename header according to the pos file names and create a merge field time\nnames(tempDB2F) = c(\"time\",\"ch1_r\",\"ch2_r\",\"ch3_r\",\"ch4_r\",\"ch5_r\",\"ch6_r\",\"ch7_r\",\"ch8_r\")\nnames(tempFC29) = c(\"time\",\"base\",\"ch1\",\"ch2\",\"ch3\",\"ch4\",\"ch5\")\n#merge files\ntemp=merge(tempFC29,tempDB2F)\n# convert datum which is a string to date format\ntemp$time=as.POSIXct(temp$time)\n# aggregate timeslots according to the value in time_period\ntemp3h = aggregateTS(as.xts(temp), alignBy = \"hours\",dropna = T,alignPeriod = time_period)\n# add the datum colum (which is now a pointer of the timeseries) as first col in the dataset\ntemp_fin=as_tibble(temp3h) %&gt;% add_column(time = index(temp3h), .before = 1)\n# transpose and combine the table\ntemp_fin=as_tibble(cbind(nms = names(temp_fin), t(temp_fin)))\n# delete first row \nnames(temp_fin) = temp_fin[1,]\ntemp_fin=temp_fin[-1,]\n# replace names specially time by stationid\nnames(temp_fin)[names(temp_fin) == 'time'] = 'stationid'\n# extract altitudes for positions\npos$altitude= exactextractr::exact_extract(DTM,st_buffer(pos,1),\"mean\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |======================================================================| 100%\n\n# merge positions and values via id\nm=merge(pos,temp_fin)\n# make the var name working for gstat by replacing all patterns\nn= gsub(x = names(m),pattern = \"-\",replacement = \"\")\nn= gsub(x = n,pattern = \" \",replacement = \"\")\nn= gsub(x = n,pattern = \":\",replacement = \"\")\nn= gsub(x = n,pattern = \"2023\",replacement = \"A2023\")\n# and rename couse this as new names\nnames(m)=n\nsaveRDS(m,cleandata)\n\n\n\nPreparing and converting spatial basis data sets\nAfter the basic cleaning is finished we prepare some specific datasets according to the technical needs.\n\n# grep the varnames for an interpolation loop\nvars=grep(glob2rx(\"A2023*\"), n, value = TRUE)\nvars\n\n[1] \"A20230828230000\" \"A20230829020000\" \"A20230829050000\" \"A20230829080000\"\n[5] \"A20230829110000\" \"A20230829140000\" \"A20230829170000\" \"A20230829200000\"\n[9] \"A20230829220000\"\n\n# convert final sf vector to terra vector\ntemperature_vect = vect(m)\ntemperature_vect \n\n class       : SpatVector \n geometry    : points \n dimensions  : 14, 11  (geometries, attributes)\n extent      : 183055.8, 183170.3, 5748366, 5748499  (xmin, xmax, ymin, ymax)\n coord. ref. : WGS 84 / UTM zone 33N (EPSG:32633) \n names       : stationid altitude A20230828230000 A20230829020000\n type        :     &lt;chr&gt;    &lt;num&gt;           &lt;chr&gt;           &lt;chr&gt;\n values      :      base    579.1            10.3            10.1\n                     ch1      578            10.6            10.2\n                   ch1_r    575.3            10.7            10.0\n A20230829050000 A20230829080000 A20230829110000 A20230829140000\n           &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;\n             9.5             9.6            14.8              13\n             9.5             9.7              12            14.3\n             9.6            10.4            13.4            18.3\n A20230829170000 A20230829200000 A20230829220000\n           &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;\n            15.2            12.8            11.5\n            16.9            12.9            10.4\n            19.2             9.9             9.5\n\n# create  template raster\nr=DTM *0\n# create table containing x, y, value  to interpolate this values in space\nxyz=cbind(geom(temperature_vect)[,3],geom(temperature_vect)[,4],as.numeric(temperature_vect$A20230829220000))\n# the same just for x,y\nxy=cbind(geom(temperature_vect)[,3],geom(temperature_vect)[,4])\n#the same just for z\nz=as.numeric(temperature_vect$A20230829220000)\n# convert to data frame and name header\nxyz=data.frame(xyz)\nnames(xyz) =c(\"x\",\"y\",\"temp\")\nxyz\n\n          x       y temp\n1  183137.7 5748385 11.5\n2  183087.0 5748438 10.4\n3  183130.3 5748437  9.5\n4  183055.8 5748460 10.7\n5  183098.0 5748374 10.6\n6  183081.0 5748499 10.6\n7  183110.5 5748389 10.6\n8  183095.5 5748465 10.7\n9  183144.5 5748366 10.0\n10 183120.1 5748476 10.5\n11 183170.3 5748415  8.4\n12 183144.8 5748427  9.0\n13 183156.8 5748389  9.0\n14 183134.0 5748399  9.2"
  },
  {
    "objectID": "mc_session/mc3.html#further-hands-on-examples",
    "href": "mc_session/mc3.html#further-hands-on-examples",
    "title": "Spatial Interpolation",
    "section": "Further Hands on examples",
    "text": "Further Hands on examples\nThe Forgenius Pinus Pinaster Project provides an fully integrated GIS source code and field data dealing with prediction classificaten of UAV and station realted data."
  }
]